---
title: "Fire severity - initial carbon"
output:
  word_document:
    fig_height: 6
    fig_width: 8
  html_document: default
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(mxbutils)

library(dplyr)
library(ggplot2)
library(reshape2)
library(stringr)

library(rjags)
load.module("glm")


# short-cut functions for quantiles
lwr <- function(x) quantile(x, probs=0.025)
upr <- function(x) quantile(x, probs=0.975)

set.seed(1234)

RUN.MODEL <- FALSE

# ggplot bits
t <- theme_bw() +
  theme(text = element_text(size = 16))

theme_set(t)

```

```{r}

DAT <- loadFrom(mdPath("data/firesev_totalc.RData"))

if (!RUN.MODEL) {
  path <- mdPath("model_outputs")
  
  # Load model predictions from earlier run
  # (if more than 1 file take the last which should be most recent)
  f <- tail( dir(path, pattern="^init_model_predictions.*RData$", full.names = TRUE), 1)
  M.predicted <- loadFrom(f)

  # Load model parameter samples
  f <- tail( dir(path, pattern="^init_model_parameters.*RData$", full.names = TRUE), 1)
  m.sims <- loadFrom(f)  
}

```


```{r}

# Function which takes an mcmc object generated with coda.samples
# and calculates a summary data frame of rates of inclusion and 
# coefficient values for each predictor.
#
# m - matrix of MCMC samples extracted from model outputs. It is 
#     assumed to have a set of columns for coefficient samples and
#     a matching set of columns for the corresponding binary indicator
#     variables.
#
makeVarSummary <- function(m, indicator.prefix = "^ind_") {
  
  indicator.cols <- which( str_detect(colnames(m), indicator.prefix) )
  var.names <- str_replace(colnames(m)[indicator.cols], indicator.prefix, "")
  var.cols <- match(var.names, colnames(m))
  
  if (anyNA(var.cols)) stop("Unmatched column names")
  
  Nterms <- length(var.cols)
  
  # Rate of variable inclusion
  var.inclusion <- apply(m[, indicator.cols], 2, mean)
  
  # Coefficient values: included and overall
  var.coefs <-
    sapply(1:Nterms, 
           function(i) {
             x <- m[, var.cols[i]]
             ind <- m[, indicator.cols[i]]
             
             x.in <- x[ ind == 1 ]
             if (length(x.in) == 0) c(0, 0, 0, 0, FALSE)
             else {
               qs <- quantile(x.in, probs=c(0.025, 0.975))
               non.zero <- sign(qs[1]) != 0 & sign(qs[1]) == sign(qs[2])
               c(mean(x.in), mean(x), qs, non.zero)
             }
           })
  
  # Create output data frame

  var.summary <- data.frame(
    Variable = var.names,
    PercentInclusion = 100 * var.inclusion,
    MeanIncluded = var.coefs[1, ],
    MeanOverall =  var.coefs[2, ],
    Lower = var.coefs[3, ],
    Upper = var.coefs[4, ],
    NonZero = var.coefs[5, ],
    stringsAsFactors = FALSE
  )
  
  arrange(var.summary, desc(PercentInclusion))
}

```


```{r}

X.formula <- formula(~ depth * severity +
                       microsite * severity)

# Model matrix for regression
X <- model.matrix(X.formula, data = DAT)
Nterms <- ncol(X)


# Constraint matrix on variable inclusion
X.constraints <- matrix(0, nrow=Nterms, ncol=Nterms)
term.names <- colnames(X)
rownames(X.constraints) <- term.names
colnames(X.constraints) <- term.names

# This recursive function takes a constraint matrix row index
# and a term name and updates the matrix for inclusion of the
# relevant component terms.
#
setConstraint <- function(X.constraints, row.index, term.name) {
  if (is.null(term.name)) {
    term.name <- term.names[row.index]
    term.index <- row.index
  } else {
    term.index <- match(term.name, term.names)
  }
  
  X.constraints[row.index, term.index] <- 1
  
  torder <- 1 + str_count(term.name, "\\:")
  
  if (torder > 1) {
    components <- combn( unlist( str_split(term.name, "\\:")), torder-1 )
    components <- apply(components, 2, paste, collapse=":")
    
    for (component in components) {
      X.constraints <- setConstraint(X.constraints, row.index, component)
    }
  }
  
  X.constraints
}

# Run the function to populate the constraint matrix.
#
for (i in 1:Nterms) {
  X.constraints <- setConstraint(X.constraints, i, NULL)
}


# Data and model matrix for posterior predictions.
# This has one row for each of the 24 possible combinations
# of the factor predictors (2 depths x 3 microsites x 4 fire severities).
#
DAT.predict <- DAT %>%
  select(depth, microsite, severity) %>%
  distinct()

X.predict <- model.matrix(X.formula, data = DAT.predict)

```


```{r eval = RUN.MODEL}

modelTxt <- "
  model {

    for (i in 1:N) { 
      yobs[i] ~ dnorm(mu[i], sigma^(-2)) 
    }

    # calculation of mean responses
    mu <- X %*% beta

    # Predictor inclusion: first draw unconstrained values
    for (i in 1:Nterms) {
      indDash[i] ~ dbern(p.ind)
    }

    # Predictor inclusion: now apply constraints to ensure
    # interaction terms are accompanied by their component terms
    # (note >0 test to reduce all non-zero terms to 1)
    ind <- (indDash %*% X.constraints) > 0

    # Predictor inclusion: finally apply indicators to 
    # coefficients
    for (i in 1:Nterms) {
      betaDash[i] ~ dnorm(0, sigma.ind^(-2))
      beta[i] <- betaDash[i] * ind[i]
    }

    p.ind ~ dbeta(1, 1)

    # Prior on common standard deviation of field samples
    sigma ~ dunif(0, 10)

    # Prior on standard deviation for variable inclusion
    sigma.ind ~ dunif(0, 10)
    
    ###########################################
    # Posterior predictions
    ###########################################
    
    mu.predict <- X.predict %*% beta
    
    for (i in 1:Npredict) { 
      ypredict[i] ~ dnorm(mu.predict[i], sigma^(-2)) 
    }  
  }
"

zz <- textConnection(modelTxt)

model <- jags.model(zz, 
                    
                    data=list(yobs = log10(DAT$percentC), 
                              X = X,
                              X.constraints = X.constraints,
                              N = nrow(DAT), 
                              Nterms = Nterms,
                              X.predict = X.predict,
                              Npredict = nrow(X.predict)),

                    inits = function(...) {
                      list(
                        betaDash = runif(Nterms, -1, 1), 
                        sigma = runif(1, 0.1, 10),
                        sigma.ind = runif(1, 0.1, 10),
                        p.ind = runif(1, 0.25, 0.75),
                        indDash = rep(0, Nterms),
                        ypredict = rep(0, nrow(X.predict)) )
                    }, 
                    
                    n.chains = 1)

close(zz)

update(model, 50000)

sims <- coda.samples(model, 
                     c("beta", "ind", "p.ind", "sigma", "ypredict"), 
                     n.iter=100000, 
                     thin=10)

# Extract two matrices from the MCMC output object:
# - matrix of model parameter samples
# - matrix of posterior predictions joined to prediction case data
#
m.sims <- as.matrix(sims[[1]])
rm(sims)
gc()
ii.predict <- str_detect(colnames(m.sims), "^ypredict")
M.predicted <- m.sims[, ii.predict]


# Transpose the matrix of predictions and join it to the
# predict case data
M.predicted <- t(M.predicted)
M.predicted <- cbind(DAT.predict, M.predicted)


# Save this matrix to the outputs dir so that we don't have to
# run the model everytime we've cleared out the workspace.
#
time <- format(Sys.time(), "%Y-%m-%d-%H-%M")
path <- mdPath("model_outputs")

save(M.predicted, 
     file = paste0(file.path(path, "init_model_predictions_"), time, ".RData"))

# Separately save the matrix of samples for other variables.
# We change 'beta' and 'ind' column names to names of predictors in X
# to make subsequent use easier, and get rid of pesky square brackets 
# in the remaining column names.

ii.beta <- str_detect(colnames(m.sims), "^beta\\[")
colnames(m.sims)[ii.beta] <- colnames(X)

ii.ind <- str_detect(colnames(m.sims), "^ind\\[")
colnames(m.sims)[ii.ind] <- paste("ind", colnames(X), sep="_")

# purge brackets from column names
colnames(m.sims) <- str_replace_all(colnames(m.sims), "\\[|\\(|\\]|\\)", "")

save(m.sims[, !ii.predict],
     file = paste0(file.path(path, "init_model_parameters_"), time, ".RData"))

```


## Variable importance

```{r}

# Generate summary of percent inclusion and coefficient values for predictors
varimp <- makeVarSummary(m.sims)
knitr::kable(varimp, digits=4)
path <- file.path(mdPath("model_outputs"), "initcarbon_varimp.csv")
write.csv(varimp, file = path, row.names = FALSE)

```


```{r}

# Column indices of predicted values in M.predicted
predict.cols <- (ncol(DAT.predict) + 1):ncol(M.predicted)

```


## Model predictions by depth, fire severity

```{r}

dat <- melt(M.predicted, 
            id.vars = c("severity", "depth"), 
            measure.vars = predict.cols) %>%
  mutate(value = 10^value)

ggplot(data = dat) + 
  geom_density(aes(x = value, colour = severity),
               size = 1) +
  
  scale_colour_discrete(name = "Fire severities") +
  scale_x_log10() +
  
  labs(x = "%CTot") +
  facet_wrap(~ depth, ncol=1)


# Tabular summary of the above predictions

x <- dat %>%
  group_by(depth, severity) %>%
  summarize(
    mean = mean(value),
    lwr = quantile(value, 0.025),
    upr = quantile(value, 0.975))


knitr::kable(x, digits=2)

```


## Model predictions by depth, micro-site, fire severity

```{r}

dat <- melt(M.predicted, 
            id.vars = c("microsite", "depth", "severity"), 
            measure.vars = predict.cols) %>%
  mutate(value = 10^value)

ggplot(data = dat) + 
  geom_density(aes(x = value, colour = severity),
                     size = 1) +
  
  scale_colour_discrete(name = "Fire severities") +
  scale_x_log10() +
  
  labs(x = "%CTot") +
  facet_grid(microsite ~ depth)


# Tabular summary of the above predictions

x <- dat %>%
  group_by(depth, microsite, severity) %>%
  summarize(
    mean = mean(value),
    lwr = quantile(value, 0.025),
    upr = quantile(value, 0.975))

knitr::kable(x, digits=2)

```


## Graph of means and intervals corresponding to above full-breakdown table

```{r}

ggplot(data = x) +
  geom_errorbar(aes(x = severity, group = microsite, ymin = lwr, ymax = upr),
                width = 0.5,
                position = position_dodge(width = 0.7)) +
  
  geom_point(aes(x = severity, y = mean, shape = microsite), 
             size = 3,
             position = position_dodge(width = 0.7)) +
  
  scale_shape(name = "micro-site", 
              breaks = c("O", "R", "S"),
              labels = c("open", "rough", "smooth")) +
  
  labs(x = "Fire severity", y = expression("%"*C[Tot])) +
  
  facet_wrap(~ depth, ncol = 1)

```

